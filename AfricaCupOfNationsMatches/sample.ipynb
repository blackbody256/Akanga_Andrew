{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be543b0",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "- Training models is resource intensive\n",
    "- training datasets can be too big to fit into the ram/memory\n",
    "**There are ways of solving this:**\n",
    "- Batch processing training incrementally\n",
    "- Sample data to fit contraints\n",
    "- Distribute training on many machines\n",
    "\n",
    "## For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1d71bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5-2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bird</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3-2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1       2\n",
       "2   dog  0.2  5-2021\n",
       "3  bird  3.3  3-2021"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df1= pd.DataFrame([\n",
    "    [\"cat\", 1.0, \"3-2021\"],\n",
    "    [\"cat\", 0.5, \"1-2021\"],\n",
    "    [\"dog\", 0.2, \"5-2021\"],\n",
    "    [\"bird\", 3.3, \"3-2021\"]])\n",
    "#return sample of df with size of 2\n",
    "df1.sample(n=2,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ca3dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[-2.22044605e-16  1.00000000e+00  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([[5, 3.4, 6], [1, 0.4, 10], [2, 0.1, 1]])\n",
    "target = [0, 1, 1]\n",
    "\n",
    "# One line model creation\n",
    "reg = LinearRegression().fit(df, target)\n",
    "\n",
    "# Score model with default metrics\n",
    "print(reg.score(df, target))\n",
    "\n",
    "#output\n",
    "# 1.0\n",
    "\n",
    "# Predict targets\n",
    "print(reg.predict(df))\n",
    "# output\n",
    "#[-2.22044605e-16  1.00000000e+00  1.00000000e+00]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4bdd9",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Metrics are used to evaluate the outcome of your model. While there are recommendations on which metric is appropriate for the model you currently use, each model object has different variations that may provide additional insight.\n",
    "\n",
    "## Regression Metrics\n",
    "- Compares predicted output values with real output values\n",
    "- The difference between predicted and real value determines the model performance\n",
    "### R2\n",
    "`R2` measures the proportion of variance between values. It is somewhat related to correlation and has a value that ranges from -1 to 1. The higher the R2 value, the better the model.\n",
    "\n",
    "### RMSE - Root Mean Squared Error\n",
    "RMSE measures the standard deviation of prediction errors to target values. Another term for this is residuals. The lower the RMSE, the better the model.\n",
    "\n",
    "## Classification Metrics\n",
    "- Compares the predicted label with the real label value\n",
    "- Options to do comparison are either 2 label classification or multiple label classification\n",
    "\n",
    "**Quick Refresher on Prediction Outcomes**\n",
    "The following metrics are calculated by how well a model can classify a dataset.\n",
    "\n",
    "The table below shows the 4 types of model results:\n",
    "Here is the information formatted as a table:\n",
    "\n",
    "|                      | **Predicted: Positive**         | **Predicted: Negative**        |\n",
    "| -------------------- | ------------------------------- | ------------------------------ |\n",
    "| **Actual: Positive** | TP – Correctly labeled as Yes   | FN – Incorrectly labeled as No |\n",
    "| **Actual: Negative** | FP – Incorrectly labeled as Yes | TN – Correctly labeled as No   |\n",
    "\n",
    "For example, you work at a publishing company, and you want your model to identify who out of 100 people have read a specific book and here are the results:\n",
    "\n",
    "- 40 TP - people who have read the book and are labeled as Yes by the model\n",
    "- 35 TN - people who have not read the book and are labeled as No by the model\n",
    "- 15 FP - people who have not read the book and are labeled as Yes by the model\n",
    "- 10 FN - people who have read the book and are labeled as No by the model\n",
    "\n",
    "## Accuracy\n",
    "Accuracy is calculated by the total correct predictions divided by the total number of data points.\n",
    "\n",
    "**Calculated by:**\n",
    "\n",
    "$$\\text{accuracy} = \\frac{TP + TN}{Total}$$\n",
    "\n",
    "Using the example above the accuracy would be calculated as follows:\n",
    "\n",
    "$$\\text{accuracy} = \\frac{40 + 35}{100} = \\frac{75}{100} = 0.75$$\n",
    "\n",
    "**Precision**\n",
    "Precision is another metric related to accuracy but explains how good the model is at identifying the relevant label. Calculated by the number of true positives divided by the number of true and false positives.\n",
    "\n",
    "$$\\text{precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Using the example above the precision would be calculated as follows:\n",
    "\n",
    "$$\\text{precision} = \\frac{40}{40 + 15} = \\frac{40}{55} = 0.7272$$\n",
    "\n",
    "**Recall**\n",
    "Measures how many relevant labels are actually selected. Calculated by the number of true positives divided by the number of true positives and false negatives.\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Using the example above the recall would be calculated as follows:\n",
    "\n",
    "$$\\text{recall} = \\frac{40}{40 + 10} = \\frac{40}{50} = 0.8$$\n",
    "\n",
    "**Trade-Offs**\n",
    "While default metrics for algorithms are good for initial evaluation, it is recommended to calculate several metrics to have a better understanding of overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccf63c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
